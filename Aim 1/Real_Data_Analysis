## Real data analysis
setwd("/Users/guchenyang/Dropbox/BART_Project")

library(dplyr)

# data preprocessing
dat0 = read.csv("dat1.csv")

#some patients are treated at diagnosis
rx0 = filter(dat0,days2trx==0) #548 patients
dat0[dat0$days2trx==0,]$days2trx = .5
#which variable has missing data
colSums(is.na(dat0))
dat = na.omit(dat0)

dat[dat$CS_SITESPECIFIC_FACTOR_8<=6,]$CS_SITESPECIFIC_FACTOR_8 = 6
dat[!dat$RACE %in% c(1,2),]$RACE = 3 #combine non-white & non-black race group
dat[dat$SPANISH_HISPANIC_ORIGIN %in% c(1:8),]$SPANISH_HISPANIC_ORIGIN = 1
dat = dat %>% mutate (tstage =ifelse(TNM_CLIN_T %in% c("3","3A","3B","4"),2,1))%>% select(-TNM_CLIN_T)

names(dat) = c("age","race","spanish","insurance","income","education","deyo","dyear","psa","gs", "surgsite", "regdose", "boostdose","surgseq","hormone","fupmonth","id","trx","days2trx","death","totdose","tstage")

dat = dat %>% mutate(dyearcat=ifelse(dyear %in% c(2004:2007), 1, ifelse(dyear %in% c(2008:2010),2,3)))

#dim(dat)
#[1] 42765    23

#proportion of death in each trx group
group_by(dat,trx) %>% summarise(n=n(), p = sum(death)/n)
#Source: local data frame [3 x 3]
#    trx     n          p
#  (int) (int)      (dbl)
#1     1 24688 0.02948801
#2     2 15435 0.09420149
#3     3  2642 0.06056018


set.seed(3847293)
#covariates
covs.cont = c("psa", "days2trx","age")
covs.cat = c("race","spanish","insurance","income","education","deyo","gs","tstage","dyearcat")
covs = c(covs.cont,covs.cat)
ncovs = length(covs)
#treatment
treat = dat$trx
#outcome
y = dat$death

nobs = length(y)

#mean(y[treat==1])
#[1] 0.02948801
#mean(y[treat==2])
#[1] 0.09420149
#mean(y[treat==3])
#[1] 0.06056018




############################################################################################
## Naive Analysis (without adjustment or modeling)
# potential outcome mean for each treatment level
y1.hat = mean(y[treat==1])
y2.hat = mean(y[treat==2])
y3.hat = mean(y[treat==3])

m1.hat = sum(y[treat==1])
m2.hat = sum(y[treat==2])
m3.hat = sum(y[treat==3])

# risk difference
RD12.est = y1.hat - y2.hat
RD12.var = y1.hat * (1-y1.hat) / n1.tilde + y2.hat * (1-y2.hat) / n2.tilde

RD13.est = y1.hat - y3.hat
RD13.var = y1.hat * (1-y1.hat) / n1.tilde + y3.hat * (1-y3.hat) / n3.tilde

RD23.est = y2.hat - y3.hat
RD23.var = y1.hat * (1-y2.hat) / n2.tilde + y3.hat * (1-y3.hat) / n3.tilde

# relative risk (log-scale)
RR12.est = log(y1.hat / y2.hat)
RR12.var = (1 - y1.hat) / y1.hat / n1.tilde + (1 - y2.hat) / y2.hat / n2.tilde

RR13.est = log(y1.hat / y3.hat)
RR13.var = (1 - y1.hat) / y1.hat / n1.tilde + (1 - y3.hat) / y3.hat / n3.tilde

RR23.est = log(y2.hat / y3.hat)
RR23.var = (1 - y2.hat) / y2.hat / n2.tilde + (1 - y3.hat) / y3.hat / n3.tilde

# marginal odds ratio (log-scale)
OR12.est = log( (y1.hat / (1 - y1.hat)) / (y2.hat / (1 - y2.hat)) )
OR12.var = 1 / m1.hat + 1 / (n1.tilde - m1.hat) + 1 / m2.hat + 1 / (n2.tilde - m2.hat)

OR13.est = log( (y1.hat / (1 - y1.hat)) / (y3.hat / (1 - y3.hat)) )
OR13.var = 1 / m1.hat + 1 / (n1.tilde - m1.hat) + 1 / m3.hat + 1 / (n3.tilde - m3.hat)

OR23.est = log( (y2.hat / (1 - y2.hat)) / (y3.hat / (1 - y3.hat)) )
OR23.var = 1 / m2.hat + 1 / (n2.tilde - m2.hat) + 1 / m3.hat + 1 / (n3.tilde - m3.hat)


Estimate_Summ = function(RD.est, RD.var, logRR.est, logRR.var, logOR.est, logOR.var) {
    # risk difference
    RD.lower = RD.est + qnorm(0.025) * sqrt(RD.var)
    RD.upper = RD.est + qnorm(0.975) * sqrt(RD.var)
    
    # relative risk
    logRR.lower = logRR.est + qnorm(0.025) * sqrt(logRR.var)
    logRR.upper = logRR.est + qnorm(0.975) * sqrt(logRR.var)
    
    RR.est = exp(logRR.est)
    RR.lower = exp(logRR.lower)
    RR.upper = exp(logRR.upper)
    
    # marginal odds ratio
    logOR.lower = logOR.est + qnorm(0.025) * sqrt(logOR.var)
    logOR.upper = logOR.est + qnorm(0.975) * sqrt(logOR.var)
    
    OR.est = exp(logOR.est)
    OR.lower = exp(logOR.lower)
    OR.upper = exp(logOR.upper)
    
    # summarize results
    RD = c(RD.est, RD.lower, RD.upper)
    RR = c(RR.est, RR.lower, RR.upper)
    OR = c(OR.est, OR.lower, OR.upper)
    
    res = rbind(RD,RR, OR)
    colnames(res) = c("EST","LOWER","UPPER")
    
    return(res)
}

summ12 = Estimate_Summ(RD12.est, RD12.var, RR12.est, RR12.var, OR12.est, OR12.var)
summ13 = Estimate_Summ(RD13.est, RD13.var, RR13.est, RR13.var, OR13.est, OR13.var)
summ23 = Estimate_Summ(RD23.est, RD23.var, RR23.est, RR23.var, OR23.est, OR23.var)
round(cbind(summ12, summ13, summ23), digits=3)
#      EST  LOWER  UPPER |    EST  LOWER  UPPER |   EST LOWER UPPER
#RD -0.065 -0.070 -0.060 | -0.031 -0.040 -0.022 | 0.034 0.024 0.043
#RR  0.313  0.287  0.341 |  0.487  0.412  0.575 | 1.556 1.328 1.822
#OR  0.292  0.267  0.320 |  0.471  0.395  0.562 | 1.613 1.363 1.910








############################################################################################
## Regression adjustment (Model-based multiple imputation)

# Imbens and Rubin, Chapter 12, Section 4.2 (The Concern with Regression Estimators)

library(arm)
mydata = cbind(y, treat, dat[,covs])

# outcome model for each treatment level
# bayesian logistic regression model, default Cauchy prior with scale 2.5
mod1 = bayesglm(y ~ psa + days2trx + age + factor(race) + spanish + factor(insurance) + factor(income) + factor(education) + factor(deyo) + factor(gs) + tstage + factor(dyearcat), data = mydata[mydata$treat==1,], family = binomial(link="logit"), x = TRUE)

mod2 = bayesglm(y ~ psa + days2trx + age + factor(race) + spanish + factor(insurance) + factor(income) + factor(education) + factor(deyo) + factor(gs) + tstage + factor(dyearcat), data = mydata[mydata$treat==2,], family = binomial(link="logit"), x = TRUE)

mod3 = bayesglm(y ~ psa + days2trx + age + factor(race) + spanish + factor(insurance) + factor(income) + factor(education) + factor(deyo) + factor(gs) + tstage + factor(dyearcat), data = mydata[mydata$treat==3,], family = binomial(link="logit"), x = TRUE)


# simulate the uncertainty in the estiamted coefficients
sim1 = sim(mod1, n.sims = 1000)
sim2 = sim(mod2, n.sims = 1000)
sim3 = sim(mod3, n.sims = 1000)

sim1.beta = coef(sim1)
sim2.beta = coef(sim2)
sim3.beta = coef(sim3)

X1.tilde = model.matrix(mod1)
X2.tilde = model.matrix(mod2)
X3.tilde = model.matrix(mod3)

n1.tilde = nrow(X1.tilde)
n2.tilde = nrow(X2.tilde)
n3.tilde = nrow(X3.tilde)

# predictive simulation using the binomial distribution
# predict potential outcomes
n.mi = n.sims = 1000
y12.tilde = array(NA, c(n.mi, n2.tilde))
y13.tilde = array(NA, c(n.mi, n3.tilde))

y21.tilde = array(NA, c(n.mi, n1.tilde))
y23.tilde = array(NA, c(n.mi, n3.tilde))

y31.tilde = array(NA, c(n.mi, n1.tilde))
y32.tilde = array(NA, c(n.mi, n2.tilde))

for (s in 1:n.sims) {
    # predict potential outcome Y1 using X2 and X3
    p12.tilde = invlogit(X2.tilde %*% sim1.beta[s,])
    y12.tilde[s,] = rbinom(n2.tilde, 1, p12.tilde)
    
    p13.tilde = invlogit(X3.tilde %*% sim1.beta[s,])
    y13.tilde[s,] = rbinom(n3.tilde, 1, p13.tilde)
    
    # predict potential outcome Y2 using X1 and X3
    p21.tilde = invlogit(X1.tilde %*% sim2.beta[s,])
    y21.tilde[s,] = rbinom(n1.tilde, 1, p21.tilde)
    
    p23.tilde = invlogit(X3.tilde %*% sim2.beta[s,])
    y23.tilde[s,] = rbinom(n3.tilde, 1, p23.tilde)
    
    # predict potential outcome Y3 using X1 and X2
    p31.tilde = invlogit(X1.tilde %*% sim3.beta[s,])
    y31.tilde[s,] = rbinom(n1.tilde, 1, p31.tilde)
    
    p32.tilde = invlogit(X2.tilde %*% sim3.beta[s,])
    y32.tilde[s,] = rbinom(n2.tilde, 1, p32.tilde)
}


# Average treatment effects (ATEs)
RD12.est = RR12.est = OR12.est = NULL
RD12.var = RR12.var = OR12.var = NULL

RD13.est = RR13.est = OR13.est = NULL
RD13.var = RR13.var = OR13.var = NULL

RD23.est = RR23.est = OR23.est = NULL
RD23.var = RR23.var = OR23.var = NULL

for (m in 1:n.mi) {
    
    # potential outcome mean for each treatment level
    y1.hat = mean(c(y[treat==1], y12.tilde[m,], y13.tilde[m,]))
    y2.hat = mean(c(y[treat==2], y21.tilde[m,], y23.tilde[m,]))
    y3.hat = mean(c(y[treat==3], y31.tilde[m,], y32.tilde[m,]))
    
    m1.hat = sum(c(y[treat==1], y12.tilde[m,], y13.tilde[m,]))
    m2.hat = sum(c(y[treat==2], y21.tilde[m,], y23.tilde[m,]))
    m3.hat = sum(c(y[treat==3], y31.tilde[m,], y32.tilde[m,]))
    
    # risk difference
    RD12.est[m] = y1.hat - y2.hat
    RD12.var[m] = y1.hat * (1-y1.hat) / n1.tilde + y2.hat * (1-y2.hat) / n2.tilde
    
    RD13.est[m] = y1.hat - y3.hat
    RD13.var[m] = y1.hat * (1-y1.hat) / n1.tilde + y3.hat * (1-y3.hat) / n3.tilde
    
    RD23.est[m] = y2.hat - y3.hat
    RD23.var[m] = y1.hat * (1-y2.hat) / n2.tilde + y3.hat * (1-y3.hat) / n3.tilde
    
    # relative risk (log-scale)
    RR12.est[m] = log(y1.hat / y2.hat)
    RR12.var[m] = (1 - y1.hat) / y1.hat / nobs + (1 - y2.hat) / y2.hat / nobs
    
    RR13.est[m] = log(y1.hat / y3.hat)
    RR13.var[m] = (1 - y1.hat) / y1.hat / nobs + (1 - y3.hat) / y3.hat / nobs
    
    RR23.est[m] = log(y2.hat / y3.hat)
    RR23.var[m] = (1 - y2.hat) / y2.hat / nobs + (1 - y3.hat) / y3.hat / nobs
    
    # marginal odds ratio (log-scale)
    OR12.est[m] = log( (y1.hat / (1 - y1.hat)) / (y2.hat / (1 - y2.hat)) )
    OR12.var[m] = 1 / m1.hat + 1 / (nobs - m1.hat) + 1 / m2.hat + 1 / (nobs - m2.hat)
    
    OR13.est[m] = log( (y1.hat / (1 - y1.hat)) / (y3.hat / (1 - y3.hat)) )
    OR13.var[m] = 1 / m1.hat + 1 / (nobs - m1.hat) + 1 / m3.hat + 1 / (nobs - m3.hat)
    
    OR23.est[m] = log( (y2.hat / (1 - y2.hat)) / (y3.hat / (1 - y3.hat)) )
    OR23.var[m] = 1 / m2.hat + 1 / (nobs - m2.hat) + 1 / m3.hat + 1 / (nobs - m3.hat)
}

# Combining rules
# multiple imputation combination rule
MICombineScalar = function(imputeMat) {
    # Arguments:
    # imputeMat: a matrix or array, the first column is estimates(Q), the second column is variance(U)
    
    numImp = nrow(imputeMat)       	# number of imputaions
    Qbar = mean(imputeMat[,1])	 	# Overall estimate of Q
    Ubar = mean(imputeMat[,2])	 	# Within-imputation variance
    B = var(imputeMat[,1])         	# Between-imputation variance
    T = Ubar + (1 + 1/numImp) * B  	# Estimated total variance
    r = (1 + 1/numImp) * B / Ubar
    v = (numImp - 1) * (1 + 1/r)^2 	# Degress of freedom
    list(Qbar=Qbar, T=T, v=v)
}

# summarize the estimates and 95% CI of all pairwise risk differences, relative risks and marginal odds ratios
# Note: when n.mi > 100, we can multiple imputation combining rules can be replaced by using percentiles of the
# estimates to obtain the credible interval.
MI_Estimate_Summ = function(RD.est, RD.var, RR.est, RR.var, OR.est, OR.var) {
    
    # risk difference
    RD.MI = MICombineScalar(cbind(RD.est, RD.var))
    RD.est = RD.MI$Qbar
    RD.lower = RD.est + qt(0.025, RD.MI$v) * sqrt(RD.MI$T)
    RD.upper = RD.est + qt(0.975, RD.MI$v) * sqrt(RD.MI$T)
    
    # relative risk
    RR.MI = MICombineScalar(cbind(RR.est, RR.var))
    logRR.est = RR.MI$Qbar
    logRR.lower = logRR.est + qt(0.025, RR.MI$v) * sqrt(RR.MI$T)
    logRR.upper = logRR.est + qt(0.975, RR.MI$v) * sqrt(RR.MI$T)
    
    RR.est = exp(logRR.est)
    RR.lower = exp(logRR.lower)
    RR.upper = exp(logRR.upper)
    
    # marginal odds ratio
    OR.MI = MICombineScalar(cbind(OR.est, OR.var))
    logOR.est = OR.MI$Qbar
    logOR.lower = logOR.est + qt(0.025, OR.MI$v) * sqrt(OR.MI$T)
    logOR.upper = logOR.est + qt(0.975, OR.MI$v) * sqrt(OR.MI$T)
    
    OR.est = exp(logOR.est)
    OR.lower = exp(logOR.lower)
    OR.upper = exp(logOR.upper)
    
    # summarize results
    RD = c(RD.est, RD.lower, RD.upper)
    RR = c(RR.est, RR.lower, RR.upper)
    OR = c(OR.est, OR.lower, OR.upper)
    
    res = rbind(RD,RR, OR)
    colnames(res) = c("EST","LOWER","UPPER")
    
    return(res)
}

summ12 = MI_Estimate_Summ(RD12.est, RD12.var, RR12.est, RR12.var, OR12.est, OR12.var)
summ13 = MI_Estimate_Summ(RD13.est, RD13.var, RR13.est, RR13.var, OR13.est, OR13.var)
summ23 = MI_Estimate_Summ(RD23.est, RD23.var, RR23.est, RR23.var, OR23.est, OR23.var)
round(cbind(summ12, summ13, summ23), digits=3)
#      EST  LOWER  UPPER |    EST  LOWER  UPPER |   EST  LOWER UPPER
#RD -0.037 -0.043 -0.030 | -0.025 -0.040 -0.010 | 0.011 -0.004 0.027
#RR  0.494  0.445  0.548 |  0.589  0.477  0.726 | 1.192  0.978 1.452
#OR  0.475  0.426  0.530 |  0.573  0.459  0.716 | 1.206  0.978 1.489










############################################################################################
## Three methods to estimate generalized propensity score (GPS)

## multinomial logit model

# https://stats.idre.ucla.edu/r/dae/multinomial-logistic-regression/
library(nnet)

mod = multinom(treat ~ psa + days2trx + age + factor(race) + spanish + factor(insurance) + factor(income) + factor(education) + factor(deyo) + factor(gs) + tstage + factor(dyearcat), data = mydata)

pred.class.probs.logit = fitted(mod)


## multinomial probit model

# Imai, K., & Van Dyk, D. A. (2005).
# MNP: R package for fitting the multinomial probit model.
# Journal of Statistical Software, 14(3), 1-32.
library(MNP)

mod.probit = mnp(treat ~ psa + days2trx + age + factor(race) + spanish + factor(insurance) + factor(income) + factor(education) + factor(deyo) + factor(gs) + tstage + factor(dyearcat), data = mydata, base = 3, n.draws = 3000, burnin = 1000, thin=20, verbose = TRUE)

pred.class.probs.mpm = predict(mod.probit, type = "prob")
pred.class.probs.probit = cbind(pred.class.probs.mpm$p[,2:3], pred.class.probs.mpm$p[,1])


## Multinomial Probit BART (MP-BART)
# Kindo, B. P., Wang, H., & Pe√±a, E. A. (2016).
# Multinomial probit Bayesian additive regression trees.
# Stat, 5(1), 119-131.
library(mpbart)

p = 3 # number of choices
Mcmc1 = list(sigma0=diag(p-1), keep=100, burn = 1000, ndraws = 2000, keep_sigma_draws = TRUE)
Prior1 = list( nu=p-1, V = .5*diag(p-1), ntrees = 200, kfac = 2.0, pbd = 1.0, pb = 0.5, alpha = 0.95, beta = 2.0, nc = 100, priorindep = FALSE, minobsnode = 10)

# response ~ choice speccific covariates | demographic covariates
out = mpbart(as.factor(treat) ~ 1 | psa + days2trx + age + factor(race) + spanish + factor(insurance) + factor(income) + factor(education) + factor(deyo) + factor(gs) + tstage + factor(dyearcat), train.data = mydata, base = 3, sep = ".", Prior = Prior1, Mcmc = Mcmc1, seedvalue = 20170713)

pred.class.probs.mpbart = out$class_prob_train




############################################################################################
## Inverse probability of treatment weighting (IPTW)
## Horvitz-Thompson estimator
library(twang)

# MP-BART
mu1.hat.mpbart = sum(y[treat==1] / pred.class.probs.mpbart[treat==1,1]) / nobs
mu2.hat.mpbart = sum(y[treat==2] / pred.class.probs.mpbart[treat==2,2]) / nobs
mu3.hat.mpbart = sum(y[treat==3] / pred.class.probs.mpbart[treat==3,3]) / nobs

RD12.est = mu1.hat.mpbart - mu2.hat.mpbart
RD13.est = mu1.hat.mpbart - mu3.hat.mpbart
RD23.est = mu2.hat.mpbart - mu3.hat.mpbart

RR12.est = mu1.hat.mpbart / mu2.hat.mpbart
RR13.est = mu1.hat.mpbart / mu3.hat.mpbart
RR23.est = mu2.hat.mpbart / mu3.hat.mpbart

OR12.est = (mu1.hat.mpbart / (1 - mu1.hat.mpbart)) / (mu2.hat.mpbart / (1 - mu2.hat.mpbart))
OR13.est = (mu1.hat.mpbart / (1 - mu1.hat.mpbart)) / (mu3.hat.mpbart / (1 - mu3.hat.mpbart))
OR23.est = (mu2.hat.mpbart / (1 - mu2.hat.mpbart)) / (mu3.hat.mpbart / (1 - mu3.hat.mpbart))

out = cbind(c(RD12.est, RR12.est, OR12.est), c(RD13.est, RR13.est, OR13.est), c(RD23.est, RR23.est, OR23.est))
colnames(out) = c("1vs2","1vs3","2vs3")
rownames(out) = c("RD","RR","OR")
round(out, digits=3)
#     1vs2  1vs3  2vs3
#RD -0.029 0.009 0.038
#RR  0.547 1.356 2.476
#OR  0.531 1.369 2.577







############################################################################################
## Inverse probability of treatment weighting (IPTW)
## Ratio estimator (in the sampling literature)

# Feng, P., Zhou, X. H., Zou, Q. M., Fan, M. Y., & Li, X. S. (2012).
# Generalized propensity score for estimating the average treatment effect of multiple treatments.
# Statistics in medicine, 31(7), 681-697.

# logit
# extremely large weights estimated from the multinomial logit model
mu1.hat.ratio.logit = sum(y[treat==1] / pred.class.probs.logit[treat==1,1]) / sum(1 / pred.class.probs.logit[treat==1,1])
mu2.hat.ratio.logit = sum(y[treat==2] / pred.class.probs.logit[treat==2,2]) / sum(1 / pred.class.probs.logit[treat==2,2])
mu3.hat.ratio.logit = sum(y[treat==3] / pred.class.probs.logit[treat==3,3]) / sum(1 / pred.class.probs.logit[treat==3,3])

RD12.est = mu1.hat.ratio.logit - mu2.hat.ratio.logit
RD13.est = mu1.hat.ratio.logit - mu3.hat.ratio.logit
RD23.est = mu2.hat.ratio.logit - mu3.hat.ratio.logit

RR12.est = mu1.hat.ratio.logit / mu2.hat.ratio.logit
RR13.est = mu1.hat.ratio.logit / mu3.hat.ratio.logit
RR23.est = mu2.hat.ratio.logit / mu3.hat.ratio.logit

OR12.est = (mu1.hat.ratio.logit / (1 - mu1.hat.ratio.logit)) / (mu2.hat.ratio.logit / (1 - mu2.hat.ratio.logit))
OR13.est = (mu1.hat.ratio.logit / (1 - mu1.hat.ratio.logit)) / (mu3.hat.ratio.logit / (1 - mu3.hat.ratio.logit))
OR23.est = (mu2.hat.ratio.logit / (1 - mu2.hat.ratio.logit)) / (mu3.hat.ratio.logit / (1 - mu3.hat.ratio.logit))

out = cbind(c(RD12.est, RR12.est, OR12.est), c(RD13.est, RR13.est, OR13.est), c(RD23.est, RR23.est, OR23.est))
colnames(out) = c("1vs2","1vs3","2vs3")
rownames(out) = c("RD","RR","OR")


# MP-BART
mu1.hat.ratio.mpbart = sum(y[treat==1] / pred.class.probs.mpbart[treat==1,1]) / sum(1 / pred.class.probs.mpbart[treat==1,1])
mu2.hat.ratio.mpbart = sum(y[treat==2] / pred.class.probs.mpbart[treat==2,2]) / sum(1 / pred.class.probs.mpbart[treat==2,2])
mu3.hat.ratio.mpbart = sum(y[treat==3] / pred.class.probs.mpbart[treat==3,3]) / sum(1 / pred.class.probs.mpbart[treat==3,3])

RD12.est = mu1.hat.ratio.mpbart - mu2.hat.ratio.mpbart
RD13.est = mu1.hat.ratio.mpbart - mu3.hat.ratio.mpbart
RD23.est = mu2.hat.ratio.mpbart - mu3.hat.ratio.mpbart

RR12.est = mu1.hat.ratio.mpbart / mu2.hat.ratio.mpbart
RR13.est = mu1.hat.ratio.mpbart / mu3.hat.ratio.mpbart
RR23.est = mu2.hat.ratio.mpbart / mu3.hat.ratio.mpbart

OR12.est = (mu1.hat.ratio.mpbart / (1 - mu1.hat.ratio.mpbart)) / (mu2.hat.ratio.mpbart / (1 - mu2.hat.ratio.mpbart))
OR13.est = (mu1.hat.ratio.mpbart / (1 - mu1.hat.ratio.mpbart)) / (mu3.hat.ratio.mpbart / (1 - mu3.hat.ratio.mpbart))
OR23.est = (mu2.hat.ratio.mpbart / (1 - mu2.hat.ratio.mpbart)) / (mu3.hat.ratio.mpbart / (1 - mu3.hat.ratio.mpbart))

out = cbind(c(RD12.est, RR12.est, OR12.est), c(RD13.est, RR13.est, OR13.est), c(RD23.est, RR23.est, OR23.est))
colnames(out) = c("1vs2","1vs3","2vs3")
rownames(out) = c("RD","RR","OR")
round(out, digits=3)
#     1vs2   1vs3  2vs3
#RD -0.044 -0.026 0.019
#RR  0.435  0.571 1.311
#OR  0.415  0.555 1.338








############################################################################################
## Bayesian Additive Regression Trees (BART)

# Hill, J. L. (2011).
# Bayesian nonparametric modeling for causal inference.
# Journal of Computational and Graphical Statistics, 20(1), 217-240.
library(dbarts)

X.tilde = rbind(X1.tilde, X2.tilde, X3.tilde)

xt = cbind(mydata$treat, X.tilde[,-1])

# treatment 1 to 2 or 3
xp1 = xt[xt[,1]==1,]
xp2 = xp1
xp3 = xp1
xp2[,1] = 2  # switch treatment label. switch 'surgery' to 'RT'
xp3[,1] = 3  # switch 'surgery' to 'RT+implants'

bart_tot12 = bart(x.train = xt, y.train = y,  x.test = xp2)
bart_tot13 = bart(x.train = xt, y.train = y,  x.test = xp3)

# treatment 2 to 1 or 3
xp2 = xt[xt[,1]==2,]
xp1 = xp2
xp3 = xp2
xp1[,1] = 1
xp3[,1] = 3

bart_tot21 = bart(x.train = xt, y.train = y,  x.test = xp1)
bart_tot23 = bart(x.train = xt, y.train = y,  x.test = xp3)

# treatment 3 to 1 or 2
xp3 = xt[xt[,1]==3,]
xp1 = xp3
xp2 = xp3
xp1[,1] = 1
xp2[,1] = 2

bart_tot31 = bart(x.train = xt, y.train = y,  x.test = xp1)
bart_tot32 = bart(x.train = xt, y.train = y,  x.test = xp2)


# Average treatment effects (ATEs)
RD12.est = RR12.est = OR12.est = NULL
RD13.est = RR13.est = OR13.est = NULL
RD23.est = RR23.est = OR23.est = NULL

for (m in 1:1000) {

    # potential outcome mean for each treatment level
    y1.hat = mean( c(y[treat==1], rbinom(n2.tilde, 1, pnorm(bart_tot21$yhat.test[m,])), rbinom(n3.tilde, 1, pnorm(bart_tot31$yhat.test[m,]))) )
    y2.hat = mean( c(y[treat==2], rbinom(n1.tilde, 1, pnorm(bart_tot12$yhat.test[m,])), rbinom(n3.tilde, 1, pnorm(bart_tot32$yhat.test[m,]))) )
    y3.hat = mean( c(y[treat==3], rbinom(n1.tilde, 1, pnorm(bart_tot13$yhat.test[m,])), rbinom(n2.tilde, 1, pnorm(bart_tot23$yhat.test[m,]))) )

    # risk difference
    RD12.est[m] = y1.hat - y2.hat
    RD13.est[m] = y1.hat - y3.hat
    RD23.est[m] = y2.hat - y3.hat
    
    # relative risk
    RR12.est[m] = y1.hat / y2.hat
    RR13.est[m] = y1.hat / y3.hat
    RR23.est[m] = y2.hat / y3.hat
    
    # marginal odds ratio (log-scale)
    OR12.est[m] = (y1.hat / (1 - y1.hat)) / (y2.hat / (1 - y2.hat))
    OR13.est[m] = (y1.hat / (1 - y1.hat)) / (y3.hat / (1 - y3.hat))
    OR23.est[m] = (y2.hat / (1 - y2.hat)) / (y3.hat / (1 - y3.hat))
}

# posterior summary
Posterior_Summ = function(RD.est, RR.est, OR.est) {
    # risk difference
    RD.mean = mean(RD.est)
    RD.lower = quantile(RD.est, probs=0.025)
    RD.upper = quantile(RD.est, probs=0.975)
    
    # relative risk
    RR.mean = mean(RR.est)
    RR.lower = quantile(RR.est, probs=0.025)
    RR.upper = quantile(RR.est, probs=0.975)
    
    # marginal odds ratio
    OR.mean = mean(OR.est)
    OR.lower = quantile(RR.mean, probs=0.025)
    OR.upper = quantile(RR.mean, probs=0.975)
    
    # summarize results
    RD = c(RD.mean, RD.lower, RD.upper)
    RR = c(RR.mean, RR.lower, RR.upper)
    OR = c(OR.mean, OR.lower, OR.upper)
    
    res = rbind(RD, RR, OR)
    colnames(res) = c("EST","LOWER","UPPER")
    
    return(res)
}

summ12 = Posterior_Summ(RD12.est, RR12.est, OR12.est)
summ13 = Posterior_Summ(RD13.est, RR13.est, OR13.est)
summ23 = Posterior_Summ(RD23.est, RR23.est, OR23.est)
round(cbind(summ12, summ13, summ23), digits=3)

#      EST  LOWER  UPPER |    EST  LOWER  UPPER |   EST LOWER UPPER
#RD -0.065 -0.068 -0.061 | -0.032 -0.038 -0.026 | 0.033 0.026 0.039
#RR  0.313  0.296  0.330 |  0.480  0.432  0.531 | 1.535 1.390 1.701
#OR  0.292  0.313  0.313 |  0.464  0.480  0.480 | 1.591 1.535 1.535


